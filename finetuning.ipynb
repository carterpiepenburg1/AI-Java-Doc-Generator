{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e72dba0-a20d-44d2-87c6-36b2c6ef4b02",
   "metadata": {},
   "source": [
    "# Converting dataset to ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77347edd401a8a36",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datasets import load_dataset   \n",
    "\n",
    "dataset = load_dataset(\"CarterPiepenburg/code-search-net-java-docgen\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9034215-530c-4c41-887a-3d81d72a8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sharegpt(dataset, merged_prompt, output_column_name, conversation_extension=1):\n",
    "    \"\"\"\n",
    "    Convert dataset to ShareGPT format with proper variable substitution\n",
    "\n",
    "    Args:\n",
    "        dataset: The source dataset\n",
    "        merged_prompt: Template string with {column_name} placeholders\n",
    "        output_column_name: Column to use as the output/completion\n",
    "        conversation_extension: Number of examples to combine into a single conversation\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "\n",
    "    for i in range(0, len(dataset), conversation_extension):\n",
    "        conversation = []\n",
    "\n",
    "        # Process each example in the current conversation window\n",
    "        for j in range(i, min(i + conversation_extension, len(dataset))):\n",
    "            example = dataset[j]\n",
    "\n",
    "            # Format the prompt by substituting variables\n",
    "            prompt = merged_prompt\n",
    "            for column in dataset.column_names:\n",
    "                if column in merged_prompt and column in example:\n",
    "                    placeholder = \"{\" + column + \"}\"\n",
    "                    prompt = prompt.replace(placeholder, str(example[column]))\n",
    "\n",
    "            # Add the human message\n",
    "            conversation.append({\n",
    "                \"from\": \"human\",\n",
    "                \"value\": prompt\n",
    "            })\n",
    "\n",
    "            # Add the assistant message\n",
    "            conversation.append({\n",
    "                \"from\": \"assistant\",\n",
    "                \"value\": example[output_column_name]\n",
    "            })\n",
    "\n",
    "        # Add the conversation to the formatted data\n",
    "        formatted_data.append({\"conversations\": conversation})\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2722b-1564-422d-b7ad-62299d5549cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code explanation\n",
    "code_explain_dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"Explain what this Java code does: {code}\",\n",
    "    output_column_name = \"response\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3b21a-a8c5-4eae-854d-ecf98a27f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_explain_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54206b18-de29-49ef-bec5-6c8b97a8aa02",
   "metadata": {},
   "source": [
    "# Initialize Model and Token Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63293d97-60b0-4843-96bf-cec7a49c5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-1.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bc74f-ece1-445c-bb16-38c1dada7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# First, convert your list to a Hugging Face Dataset\n",
    "code_explain_dataset_hf = Dataset.from_list(code_explain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224b024-6e17-44cd-897e-d8aa62c84335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(code_explain_dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b7cd7-5e9e-42f4-b171-82b436d29a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import apply_chat_template\n",
    "chat_template = \"\"\"\n",
    "{SYSTEM}\n",
    "USER: {INPUT}\n",
    "ASSISTANT: {OUTPUT}\"\"\"\n",
    "\n",
    "default_system_message = \"\"\"\n",
    "You are generating brief documentation for a Java code snippet.\n",
    "Your response MUST be a single paragraph with NO bulletpoints, NO line breaks, and NO section headers.\n",
    "Do NOT explain the prompt. Just output the summary.\n",
    "Keep your explanation short and focused. Avoid repetition.\n",
    "Start your response with (This function)\n",
    "Summarize ONLY the core logic and purpose of the code.\n",
    "Here is the Java code:\n",
    "Summary (one paragraph only):\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use this system message with the apply_chat_template function\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    default_system_message = default_system_message\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1389f91accc4a1a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932e6b0-786d-45fd-9a98-d40f848c3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18d52f259ad87d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62548e9ee137415a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.3)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a9844d23e4729",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-1.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc19a75b3d48821",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\" ,],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",\n",
    "    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217dcb72e2d5208",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        #warmup_steps = 5,\n",
    "        warmup_steps = 10,\n",
    "        #max_steps = 10,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"ft-outputs\",\n",
    "        report_to = [],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5774a82c531ac8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.save_model(\"ft-outputs\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103a4547786d4ab6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ft-outputs\")\n",
    "tokenizer.save_pretrained(\"ft-outputs\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5562503de6da0e89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a852b81e23348",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# After training is complete\n",
    "training_logs = trainer.state.log_history\n",
    "# Extract loss values\n",
    "train_losses = [log.get('loss') for log in training_logs if 'loss' in log]\n",
    "steps = list(range(len(train_losses)))\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, train_losses)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e2dfb3785eb442b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeedd4ef7548b565",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from transformers import TextStreamer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from safetensors import safe_open\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f40d4973526fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer from your saved directory\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"ft-outputs\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=torch.float16, # You can use bfloat16 if supported by your hardware\n",
    "    load_in_4bit=True, # Assuming you're using the same quantization as during training\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17403a9b9a5a58",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "{SYSTEM}\n",
    "USER: {INPUT}\n",
    "ASSISTANT: {OUTPUT}\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are generating brief documentation for a Java code snippet.\n",
    "Your response MUST be a single paragraph with NO bulletpoints, NO line breaks, and NO section headers.\n",
    "Do NOT explain the prompt. Just output the summary.\n",
    "Keep your explanation short and focused. Avoid repetition.\n",
    "Start your response with (This function)\n",
    "Summarize ONLY the core logic and purpose of the code.\n",
    "Here is the Java code:\n",
    "Summary (one paragraph only):\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731ac8b0438c34d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to run inference on a single example\n",
    "def generate_explanation(code_to_explain):\n",
    "    # Prepare user message with Java code for explanation\n",
    "    user_message = f\"Explain what this Java code does: {code_to_explain}\"\n",
    "\n",
    "    # Create the messages list with the system message and user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template to format the input consistently with how you trained\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        chat_template = chat_template,\n",
    "        default_system_message = system_message\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate response with appropriate parameters for code generation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            streamer=None,  # No streaming for batch processing\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "\n",
    "    # Decode the output, skipping the prompt\n",
    "    generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bebbd39cae68c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process examples from rows 700 to 1000\n",
    "results = []\n",
    "start_idx = 0\n",
    "end_idx = 50\n",
    "\n",
    "\n",
    "# Convert to list if it's a Dataset object to ensure proper indexing\n",
    "if hasattr(test_dataset, 'to_list'):\n",
    "    dataset_list = test_dataset.to_list()\n",
    "else:\n",
    "    dataset_list = test_dataset\n",
    "\n",
    "# Make sure we don't exceed the dataset length\n",
    "end_idx = min(end_idx, len(dataset_list) - 1)\n",
    "subset = dataset_list[start_idx:end_idx + 1]\n",
    "\n",
    "print(f\"Processing examples from index {start_idx} to {end_idx}\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    # Extract the code from the user message\n",
    "    if \"conversations\" in example:\n",
    "        user_message = example[\"conversations\"][0][\"content\"]\n",
    "        # Extract just the code part (assuming it starts after \"Explain what this Python code does: \")\n",
    "        if \"Explain what this Java code does: \" in user_message:\n",
    "            code_part = user_message.split(\"Explain what this Java code does: \")[1]\n",
    "        else:\n",
    "            code_part = user_message  # If no prefix, use the whole message\n",
    "\n",
    "        # Generate explanation for this code\n",
    "        explanation = generate_explanation(code_part)\n",
    "\n",
    "        # Get the reference explanation\n",
    "        reference_explanation = example[\"conversations\"][1][\"content\"] if len(example[\"conversations\"]) > 1 else \"\"\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"index\": start_idx + i,\n",
    "            \"code\": code_part,\n",
    "            \"reference_explanation\": reference_explanation,\n",
    "            \"generated_explanation\": explanation\n",
    "        })\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed example {start_idx + i} ({i + 1}/{len(subset)})\")\n",
    "\n",
    "        # Print the explanation\n",
    "        print(f\"Original code: {code_part[:100]}...\")\n",
    "        print(f\"Generated explanation: {explanation}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Save raw results to a file\n",
    "with open(\"evaluation_results/inference_results_0_10.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Raw results saved to evaluation_results/inference_results_0_10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17180768fc925043",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
